{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Task VIII QML-HEP :  Vision transformer/Quantum Vision Transformer\n<br>\n\nImplement a classical Vision transformer and apply it to MNIST. Show its performance on the test data. Comment on potential ideas to extend this classical vision transformer architecture to a quantum vision transformer and sketch out the architecture in detail.\n\n","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\nfrom transformers import ViTModel, ViTConfig","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load MNIST dataset \ntransform = transforms.Compose([\n    transforms.Resize((32, 32)),  \n    transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,))\n])\n\ntrain_dataset = torchvision.datasets.MNIST(root=\"./data\", train=True, transform=transform, download=True)\ntest_dataset = torchvision.datasets.MNIST(root=\"./data\", train=False, transform=transform, download=True)\n\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define Vision Transformer model \nclass ViTClassifier(nn.Module):\n    def __init__(self, num_classes=10):\n        super(ViTClassifier, self).__init__()\n        self.vit = ViTModel(ViTConfig(image_size=32, num_labels=num_classes))  # Adjusted image size\n        self.classifier = nn.Linear(self.vit.config.hidden_size, num_classes)\n    \n    def forward(self, x):\n        outputs = self.vit(pixel_values=x).last_hidden_state[:, 0, :]\n        return self.classifier(outputs)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize model, loss function, and optimizer\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = ViTClassifier().to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-4)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Training model\nepochs = 5\nfor epoch in range(epochs):\n    model.train()\n    total_loss = 0\n    for images, labels in train_loader:\n        images, labels = images.to(device), labels.to(device)\n        images = images.repeat(1, 3, 1, 1)  # Convert grayscale to RGB\n        \n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    \n    print(f\"Epoch {epoch+1}, Loss: {total_loss/len(train_loader):.4f}\")\n\n# Evaluation\nmodel.eval()\ncorrect = 0\ntotal = 0\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images, labels = images.to(device), labels.to(device)\n        images = images.repeat(1, 3, 1, 1)\n        outputs = model(images)\n        _, predicted = torch.max(outputs, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\nprint(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-30T17:43:44.247675Z","iopub.execute_input":"2025-03-30T17:43:44.248104Z","iopub.status.idle":"2025-03-30T20:26:34.612174Z","shell.execute_reply.started":"2025-03-30T17:43:44.248071Z","shell.execute_reply":"2025-03-30T20:26:34.610500Z"}},"outputs":[{"name":"stdout","text":"Epoch 1, Loss: 0.2887\nEpoch 2, Loss: 0.1393\nEpoch 3, Loss: 0.1214\nEpoch 4, Loss: 0.1044\nEpoch 5, Loss: 0.1002\nTest Accuracy: 97.12%\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"**Comment on potential ideas to extend this classical vision transformer architecture to a quantum vision transformer and sketch out the architecture in detail.**\n<br>\nA quantum Vision Transformer (QViT) can surely provide more efficency and effectiveness using its quantum computing principles like superposition, entanglement, quantum parellelism and mechanism, etc. I have worked with transformers and they are super slow and exhausting but QViT is here for rescue!\n<br>\nQViT have benefits like **exponential speedup, efficent learning, better feature representation, etc, better generalization, memory efficiency**, etc.\n<br>\n<br>\n**Sketch of QViT architecture:** <br>\n* Quantum Data Encoding: Translating Images into Quantum States.  Quantum computers don’t process regular pixel values. Instead, they work with quantum states, so we must first convert our image into a form that a quantum processor can understand. Instead of storing pixel values as numbers, we encode them into quantum amplitudes or angles of qubits:\n<br>Amplitude Encoding: Represents pixel values as quantum state probabilities (efficient but hard to implement).<br>\nAngle Encoding: Maps pixel values to rotation angles of qubits (simpler and more hardware-friendly).\n<br>\n* Quantum Patch Embeddings: Converting Image Patches into Quantum Circuits. In classical ViTs, an image is divided into small square patches, which are then flattened and projected into a feature space. In QViT, we use quantum circuits instead. \n<br>\nBenefit: Instead of manually learning filters like in CNNs, quantum operations naturally mix and entangle features, making feature extraction more efficient.\n<br>\n* Quantum Self-Attention Mechanism: Understanding Patch Relationships. Self-attention is the heart of transformers—it determines how important each patch is relative to others. Instead of traditional matrix multiplications, QViT uses quantum operations to compute attention more efficiently.\nUse Parameterized Quantum Circuits (PQCs) to process information across all patches in parallel.\n Benefit: This enables faster and more memory-efficient attention mechanisms because quantum states store relationships intrinsically, rather than computing them explicitly.\n<br>\n* Hybrid Quantum-Classical MLP Head: Making the Final Decision . Since quantum computers are still limited in size, we use a hybrid approach where:  The early layers (feature extraction & attention) are quantum-based and The final classification is done with a classical neural network or a simple quantum classifier.\n<br>\n Benefit: This combines the best of both worlds—quantum efficiency for feature extraction and classical robustness for decision-making.","metadata":{}},{"cell_type":"markdown","source":"**Why our used approach in this task is best?**\n* Efficient Image Processing\n* Fixing Grayscale Input for ViT\n* Using a Vision Transformer (ViT) Backbone\n* Optimized Training Process\n* Fast and Good Accuracy of results!","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}